{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "class LinkedinCrawler:\n",
    "    HOMEPAGE_URL = \"https://www.linkedin.com\"\n",
    "    LOGIN_URL = \"https://www.linkedin.com/uas/login-submit\"\n",
    "\n",
    "    def __init__(self, login_email: str, login_password: str, jobs_search_url: str):\n",
    "        self.login_email = login_email\n",
    "        self.login_password = login_password\n",
    "        self.jobs_search_url = jobs_search_url\n",
    "        self._client = requests.Session()\n",
    "        self._html = self._client.get(self.HOMEPAGE_URL).content\n",
    "        self._soup = BeautifulSoup(self._html, \"html.parser\")\n",
    "        self._csrf = self._soup.find(\"input\", {\"name\": \"loginCsrfParam\"}).get(\"value\")\n",
    "        self._login_information = {\n",
    "            \"session_key\": self.login_email,\n",
    "            \"session_password\": self.login_password,\n",
    "            \"loginCsrfParam\": self._csrf,\n",
    "            \"trk\": \"guest_homepage-basic_sign-in-submit\",\n",
    "        }\n",
    "        self._login = self._client.post(self.LOGIN_URL, data=self._login_information)\n",
    "\n",
    "    def get_jobs_list_from_search(self) -> List[Dict]:\n",
    "        response = self._client.get(self.jobs_search_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all job listings\n",
    "        jobs = soup.find_all('div', class_='base-card')\n",
    "\n",
    "        jobs_list = []\n",
    "        for job in jobs:\n",
    "            title_element = job.find('h3', class_='base-search-card__title')\n",
    "            title_element = title_element.text.strip() if title_element else \"No Job Title\"\n",
    "            link_element = job.find('a', class_='base-card__full-link')\n",
    "            link_element = link_element.get('href') if link_element else \"No Job Link\"\n",
    "            jobs_list.append({\"title\": title_element, \"link\": link_element})\n",
    "        return jobs_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    linkedin_email = os.environ[\"linkedin_email\"]\n",
    "    linkedin_password = os.environ[\"linkedin_password\"]\n",
    "    jobs_search_daily = \"https://www.linkedin.com/jobs/search/?currentJobId=3908335123&f_TPR=r86400&f_WT=2&geoId=92000000&keywords=(%22Data%20Engineer%22%20OR%20%22Senior%20Data%20Engineer%22%20OR%20%22AWS%20Data%20Engineer%22)&location=Worldwide&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true\"\n",
    "\n",
    "    linkedin_crawler = LinkedinCrawler(linkedin_email, linkedin_password)\n",
    "    jobs = linkedin_crawler.get_jobs_list_from_search(jobs_search_daily)\n",
    "    print(f\"{len(jobs)}\")\n",
    "    print(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Response\n",
    "response = linkedin_crawler._client.get(jobs_search_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "number_of_jobs = soup.find_all('div', class_='scaffold-layout__list-detail-inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(number_of_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export this content to a file\n",
    "with open('jobs.html', 'w') as file:\n",
    "    file.write(str(response.content))\n",
    "# response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
